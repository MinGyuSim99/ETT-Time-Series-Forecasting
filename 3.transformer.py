# ==================================================================
# Transformer ëª¨ë¸ ë‹¨ë… ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (ì˜¤ë¥˜ ìˆ˜ì • ìµœì¢…ë³¸)
# ==================================================================
# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì œê³µëœ Transformer.pyì˜ ëª¨ë“  ë¡œì§ì„ í¬í•¨í•˜ë©°,
# ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ í›ˆë ¨, ì˜ˆì¸¡, í‰ê°€, ì‹œê°í™”ê¹Œì§€ ìˆ˜í–‰í•©ë‹ˆë‹¤.
#
# ì‹¤í–‰ ì „ ì¤€ë¹„ë¬¼:
# - ETTh1.csv
# - sample_submit.csv
# ==================================================================

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ë° ëœë¤ ì‹œë“œ ê³ ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime, timedelta
import math
import os
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

torch.manual_seed(42)
np.random.seed(42)

print("âœ… [1ë‹¨ê³„] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì‹œë“œ ê³ ì • ì™„ë£Œ.")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ê²½ë¡œ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# ëª¨ë“  íŒŒì¼ì´ ì €ì¥ë˜ê³  ë¡œë“œë  ê¸°ë³¸ ê²½ë¡œ
BASE_PATH = os.getcwd()
output_directory = os.path.join(BASE_PATH, "transformer_results/")
os.makedirs(output_directory, exist_ok=True)
print(f"ğŸ“‚ ìµœì¢… ê²°ê³¼ë¬¼ì€ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥ë©ë‹ˆë‹¤: {output_directory}")


# ì…ë ¥ íŒŒì¼ ê²½ë¡œ ì •ì˜
ETTH1_PATH = os.path.join(BASE_PATH, "ETTh1.csv")
SUBMIT_SAMPLE_PATH = os.path.join(BASE_PATH, "sample_submit.csv")

# ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì •ì˜
MODEL_SAVE_PATH = os.path.join(output_directory, "best_transformer_model.pth")
SUBMISSION_CSV_PATH = os.path.join(output_directory, "submission_transformer.csv")
HIST_IMG_PATH = os.path.join(output_directory, "histogram_transformer.png")

print("âœ… [2ë‹¨ê³„] ëª¨ë“  íŒŒì¼ ê²½ë¡œ ì„¤ì • ì™„ë£Œ.")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ë°ì´í„° ë¡œë“œ ë° íŠ¹ì„± ê³µí•™
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
try:
    # ì›ë³¸ ë°ì´í„°ëŠ” 'df' ë¼ëŠ” ë³€ìˆ˜ëª…ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
    df = pd.read_csv(ETTH1_PATH)
    submit_df_raw = pd.read_csv(SUBMIT_SAMPLE_PATH)
except FileNotFoundError as e:
    print(f"âŒ íŒŒì¼ ì˜¤ë¥˜: {e}")
    print("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— ETTh1.csvì™€ sample_submit.csv íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit()

df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

df_feat = df.copy()
df_feat['hour']    = df_feat.index.hour
df_feat['weekday'] = df_feat.index.weekday

def cyclical_encode(vals, max_val):
    radians = 2.0 * math.pi * vals / max_val
    return np.sin(radians), np.cos(radians)

sin_h, cos_h   = cyclical_encode(df_feat['hour'].values, 24)
sin_wd, cos_wd = cyclical_encode(df_feat['weekday'].values, 7)
df_feat['sin_hour'], df_feat['cos_hour'] = sin_h, cos_h
df_feat['sin_wd'], df_feat['cos_wd']     = sin_wd, cos_wd

for window in [24, 48, 168]:
    df_feat[f'OT_rollmean_{window}'] = df_feat['OT'].rolling(window).mean().bfill()
    df_feat[f'OT_rollstd_{window}']  = df_feat['OT'].rolling(window).std().bfill()
    df_feat[f'OT_lag_{window}']      = df_feat['OT'].shift(window).bfill()
    df_feat[f'OT_diff_{window}']     = df_feat['OT'] - df_feat['OT'].shift(window).bfill()

feature_cols = [
    'HUFL','HULL','MUFL','MULL','LUFL','LULL','OT',
    'sin_hour','cos_hour','sin_wd','cos_wd',
    'OT_rollmean_24','OT_rollmean_48','OT_rollmean_168',
    'OT_rollstd_24','OT_rollstd_48','OT_rollstd_168',
    'OT_lag_24','OT_lag_48','OT_lag_168',
    'OT_diff_24','OT_diff_48','OT_diff_168',
]
# ìƒì„±ëœ íŠ¹ì„±ë§Œ ì‚¬ìš©í•˜ë„ë¡ DataFrame í•„í„°ë§
df_feat = df_feat[feature_cols]

print("âœ… [3ë‹¨ê³„] íŠ¹ì„± ê³µí•™ ì™„ë£Œ.")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ë°ì´í„° ì •ê·œí™” ë° ë¶„í• 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(df_feat)
scaled_df = pd.DataFrame(data_scaled, index=df_feat.index, columns=feature_cols)

# ìŠ¤ì¼€ì¼ëœ OTì˜ ë¶„ìœ„ìˆ˜ ê¸°ì¤€ ì„ê³„ê°’ ê³„ì‚°
low_thr_scaled, high_thr_scaled = np.percentile(scaled_df['OT'], [20, 80])
print(f"ğŸ“Š ê°€ì¤‘ ì†ì‹¤(WeightedLoss) ì„ê³„ê°’: 20%={low_thr_scaled:.3f} | 80%={high_thr_scaled:.3f}")

# í•™ìŠµ/ê²€ì¦ ë¶„í•  ê¸°ì¤€ ì„¤ì • (Leak ë°©ì§€)
last_label_date = datetime(2018,1,31,23,0)
max_t0_date     = last_label_date - timedelta(hours=95)
df_trainval = scaled_df[:last_label_date]

input_len = 172
pred_len  = 96

print("âœ… [4ë‹¨ê³„] ë°ì´í„° ì •ê·œí™” ë° ë¶„í•  ì™„ë£Œ.")


#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ëª¨ë¸ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

class TS_Dataset(Dataset):
    def __init__(self, df, input_len, pred_len, feature_cols, max_t0_date, last_label_date):
        self.feat = df[feature_cols].values
        self.input_len, self.pred_len = input_len, pred_len
        self.feature_cols = feature_cols
        last_label_idx = df.index.get_loc(last_label_date)
        max_t0_idx = df.index.get_loc(max_t0_date)
        self.start_idx = [i for i in range(self.input_len, max_t0_idx + 1) if (i + self.pred_len - 1) <= last_label_idx]
    def __len__(self): return len(self.start_idx)
    def __getitem__(self, idx):
        i = self.start_idx[idx]
        x = self.feat[i - self.input_len : i]
        y = self.feat[i : i + self.pred_len, self.feature_cols.index('OT')]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

def mark_high_windows(start_idx_list, feat_array, input_len, high_cut):
    flags = []
    for i in start_idx_list:
        win = feat_array[i - input_len : i, feature_cols.index('OT')]
        flags.append( (win >= high_cut).any() )
    return np.array(flags)

class AsymWeightedMSE(nn.Module):
    def __init__(self, low_thr, high_thr, w_low=0.2, w_high=2.0):
        super().__init__()
        self.low_thr, self.high_thr = low_thr, high_thr
        self.w_low, self.w_high = w_low, w_high
    def forward(self, pred, target):
        w = torch.ones_like(target)
        w[target < self.low_thr] = self.w_low
        w[target >= self.high_thr] = self.w_high
        return (w * (pred - target) ** 2).mean()

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))
    def forward(self, x):
        return x + self.pe[:, :x.size(1), :].to(x.device)

class TransformerModel(nn.Module):
    def __init__(self, feature_dim, d_model=256, nhead=4, num_layers=2, dim_feed=256, dropout=0.2, pred_len=96):
        super().__init__()
        self.input_proj = nn.Sequential(nn.Linear(feature_dim, d_model), nn.Dropout(dropout))
        self.layer_norm = nn.LayerNorm(d_model)
        self.pos_enc = PositionalEncoding(d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feed, dropout=dropout, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.output_fc = nn.Sequential(nn.Dropout(dropout), nn.Linear(d_model, pred_len))
    def forward(self, x):
        x = self.layer_norm(self.input_proj(x))
        x = self.pos_enc(x)
        x = self.transformer(x)
        return self.output_fc(x[:, -1, :])

print("âœ… [5ë‹¨ê³„] ëª¨ë¸ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ.")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. í•™ìŠµ/ê²€ì¦ DataLoader ìƒì„± + Oversampling
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
full_ds   = TS_Dataset(df_trainval, input_len, pred_len, feature_cols, max_t0_date, last_label_date)
n_total   = len(full_ds)
n_train   = int(n_total * 0.8)
train_idx = list(range(n_train))
val_idx   = list(range(n_train, n_total))

train_ds  = Subset(full_ds, train_idx)
val_ds    = Subset(full_ds, val_idx)

# ê³ ì˜¨ ìœˆë„ìš° í”Œë˜ê·¸ ê³„ì‚°
high_flags = mark_high_windows(train_idx, full_ds.feat, input_len, high_thr_scaled)
# ìƒ˜í”Œ ê°€ì¤‘ì¹˜: ê³ ì˜¨=2, ë‚˜ë¨¸ì§€=1
sample_weights = np.where(high_flags, 2.0, 1.0)
# WeightedRandomSampler ì •ì˜
sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)

# DataLoader ìƒì„±
batch_size = 32
train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, drop_last=True)
val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)

print("âœ… [6ë‹¨ê³„] ê°€ì¤‘ ìƒ˜í”Œë§ì„ ì ìš©í•œ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ.")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. Transformer ëª¨ë¸ í›ˆë ¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
transformer = TransformerModel(feature_dim=len(feature_cols), pred_len=pred_len).to(device)
optimizer = torch.optim.AdamW(transformer.parameters(), lr=5e-5, weight_decay=1e-5)
criterion = AsymWeightedMSE(low_thr_scaled, high_thr_scaled, w_low=0.2, w_high=2.0)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6)

num_epochs = 40
early_patience = 25
best_val_loss = float('inf')
early_stop_counter = 0

print("\n" + "="*80 + "\nâ–¶ PART 7: Transformer ëª¨ë¸ í›ˆë ¨ ì‹œì‘\n" + "="*80)
for epoch in range(1, num_epochs + 1):
    transformer.train(); train_losses = []
    for x_b, y_b in train_loader:
        x_b, y_b = x_b.to(device), y_b.to(device); optimizer.zero_grad()
        loss = criterion(transformer(x_b), y_b); loss.backward()
        torch.nn.utils.clip_grad_norm_(transformer.parameters(), max_norm=1.0); optimizer.step()
        train_losses.append(loss.item())
    avg_tr_loss = np.mean(train_losses)

    transformer.eval(); val_losses = []
    with torch.no_grad():
        for x_b, y_b in val_loader:
            val_losses.append(criterion(transformer(x_b.to(device)), y_b.to(device)).item())
    avg_val_loss = np.mean(val_losses)

    print(f"  [Ep {epoch:02d}] Train Loss: {avg_tr_loss:.6f} | Val Loss: {avg_val_loss:.6f}")

    scheduler.step(avg_val_loss)
    if avg_val_loss < best_val_loss - 1e-6:
        best_val_loss = avg_val_loss
        torch.save(transformer.state_dict(), MODEL_SAVE_PATH)
        early_stop_counter = 0
    else:
        early_stop_counter += 1
        if early_stop_counter >= early_patience:
            print(f"  -- ì¡°ê¸° ì¢…ë£Œ at epoch {epoch} --")
            break

print(f"\nâœ… [7ë‹¨ê³„] ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ. ìµœì¢… ê²€ì¦ ì†ì‹¤: {best_val_loss:.6f}")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8. ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥/í‰ê°€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
print("\n" + "="*80 + "\nâ–¶ PART 8: ì˜ˆì¸¡ ë° ê²°ê³¼ í‰ê°€ ì‹œì‘\n" + "="*80)

results = []
best_model = TransformerModel(feature_dim=len(feature_cols), pred_len=pred_len).to(device)
best_model.load_state_dict(torch.load(MODEL_SAVE_PATH))
best_model.eval()

# 'OT' ì»¬ëŸ¼ì˜ ì›ë˜ ìŠ¤ì¼€ì¼ ì •ë³´ë¥¼ ì—­ë³€í™˜ì— ì‚¬ìš©
ot_col_index = feature_cols.index('OT')
ot_min, ot_max = scaler.data_min_[ot_col_index], scaler.data_max_[ot_col_index]

for _, row in submit_df_raw.iterrows():
    t0 = pd.to_datetime(row[0])
    input_end   = t0 - timedelta(hours=1)
    input_start = input_end - timedelta(hours=input_len - 1)

    seq = scaled_df.loc[input_start:input_end, feature_cols].values
    x_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0).to(device)
    with torch.no_grad():
        pred_scaled = best_model(x_tensor).cpu().numpy().flatten()
    
    pred_unnorm = pred_scaled * (ot_max - ot_min) + ot_min
    results.append([row[0]] + pred_unnorm.tolist())

col_names = [submit_df_raw.columns[0]] + [f"T{i}" for i in range(pred_len)]
df_submit = pd.DataFrame(results, columns=col_names)
df_submit.to_csv(SUBMISSION_CSV_PATH, index=False)
print(f"âœ… ì˜ˆì¸¡ íŒŒì¼ ìƒì„± ì™„ë£Œ: {SUBMISSION_CSV_PATH}")

# --- ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™” ---
y_pred_flat = df_submit.drop(columns=[submit_df_raw.columns[0]]).values.flatten()
submit_times = pd.to_datetime(df_submit.iloc[:,0].values)
gt_ots = []
for t in submit_times:
    gt_range = pd.date_range(start=t, periods=pred_len, freq='H')
    # --- ìˆ˜ì •ëœ ë¶€ë¶„ ---
    # 'df_raw' ëŒ€ì‹  'df'ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë‹µ ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    gt_series = df.reindex(gt_range)['OT']
    # --- ìˆ˜ì • ë ---
    gt_ots.append(gt_series.values)
gt_ots_flat = np.concatenate(gt_ots)
valid_indices = ~np.isnan(gt_ots_flat)
gt_ots_clean = gt_ots_flat[valid_indices]
y_pred_clean = y_pred_flat[valid_indices]

# ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
mae = mean_absolute_error(gt_ots_clean, y_pred_clean)
mse = mean_squared_error(gt_ots_clean, y_pred_clean)
rmse = np.sqrt(mse)
r2 = r2_score(gt_ots_clean, y_pred_clean)
print("\n  ã€ ìµœì¢… ì„±ëŠ¥ ì§€í‘œ ã€‘")
print(f"  - MSE  : {mse:.4f} (Kaggle í‰ê°€ì§€í‘œ)")
print(f"  - MAE  : {mae:.4f}")
print(f"  - RMSE : {rmse:.4f}")
print(f"  - RÂ²   : {r2:.4f}")

# íˆìŠ¤í† ê·¸ë¨ ìƒì„±
plt.figure(figsize=(12, 7))
plt.hist(gt_ots_clean, bins=50, alpha=0.7, label='Ground Truth', color='royalblue', density=True)
plt.hist(y_pred_clean, bins=50, alpha=0.7, label='Prediction', color='orangered', density=True)
plt.title("Transformer Prediction vs. Ground Truth Distribution", fontsize=16)
plt.xlabel("OT Value", fontsize=12); plt.ylabel("Density", fontsize=12)
plt.legend(); plt.grid(axis='y', linestyle='--', alpha=0.7); plt.tight_layout()
plt.savefig(HIST_IMG_PATH)
plt.close()
print(f"\nâœ… ë¹„êµ íˆìŠ¤í† ê·¸ë¨ ì €ì¥ ì™„ë£Œ: {HIST_IMG_PATH}")

print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ‰")