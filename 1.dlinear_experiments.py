# ==================================================================
# dl3.py ë‹¨ë… ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (ê²½ë¡œ ì„¤ì • ìˆ˜ì • ë²„ì „)
# ==================================================================
# ì›ë³¸ ì½”ë“œì—ì„œ íŒŒì¼ ê²½ë¡œ ë¶€ë¶„ë§Œ ì‹¤í–‰ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ë³€ê²½í•œ ë²„ì „ì…ë‹ˆë‹¤.
#
# ì‹¤í–‰ ì „ ì¤€ë¹„ë¬¼:
# - ì´ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ í´ë”ì— ETTh1.csv, sample_submit.csv íŒŒì¼ ìœ„ì¹˜
# ==================================================================

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì‹œë“œ ê³ ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from datetime import datetime, timedelta
import math
import os
import matplotlib.pyplot as plt

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
torch.manual_seed(42)
np.random.seed(42)
print("âœ… [1ë‹¨ê³„] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ.")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. íŒŒì¼ ê²½ë¡œ ë° ë””ë ‰í† ë¦¬ ì„¤ì • (ìƒëŒ€ ê²½ë¡œ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# --- ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘ ---
# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë˜ëŠ” ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
BASE_PATH = os.getcwd()
data_path = os.path.join(BASE_PATH, "ETTh1.csv")
sample_submission_path = os.path.join(BASE_PATH, "sample_submit.csv")

# ëª¨ë“  ê²°ê³¼ê°€ ì €ì¥ë  ë””ë ‰í† ë¦¬ ('dlinear_all_experiments_results')ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
output_directory = os.path.join(BASE_PATH, "dlinear_all_experiments_results/")
os.makedirs(output_directory, exist_ok=True)

print(f"ğŸ“‚ ëª¨ë“  ì¶œë ¥ íŒŒì¼ì€ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥ë©ë‹ˆë‹¤: {output_directory}")
# --- ìˆ˜ì •ëœ ë¶€ë¶„ ë ---


#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° íŠ¹ì„± ê³µí•™ (*** ì§€ëŠ¥í˜• íŠ¹ì„± ì¶”ê°€ ***)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
df = pd.read_csv(data_path)
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

df_feat = df.copy()
# --- ê¸°ì¡´ íŠ¹ì„± ê³µí•™ ---
df_feat['hour'] = df_feat.index.hour
df_feat['weekday'] = df_feat.index.weekday
df_feat['day_of_year'] = df_feat.index.dayofyear

def cyclical_encode(vals, max_val):
    radians = 2.0 * math.pi * vals / max_val
    return np.sin(radians), np.cos(radians)

sin_h, cos_h = cyclical_encode(df_feat['hour'].values, 24)
sin_wd, cos_wd = cyclical_encode(df_feat['weekday'].values, 7)
sin_doy, cos_doy = cyclical_encode(df_feat['day_of_year'].values, 366)
df_feat['sin_hour'], df_feat['cos_hour'] = sin_h, cos_h
df_feat['sin_wd'], df_feat['cos_wd'] = sin_wd, cos_wd
df_feat['sin_doy'], df_feat['cos_doy'] = sin_doy, cos_doy

# --- (ì‹ ê·œ) ì§€ëŠ¥í˜• íŠ¹ì„± ê³µí•™ ---
# 1. ì‹œê°„ëŒ€ íŠ¹ì„±
bins = [0, 6, 12, 18, 23]
labels = ['Night', 'Morning', 'Afternoon', 'Evening']
df_feat['time_of_day'] = pd.cut(df_feat['hour'], bins=bins, labels=labels, right=False, ordered=False)
df_feat.loc[df_feat['hour'] == 23, 'time_of_day'] = 'Evening' # 23ì‹œë¥¼ ì €ë…ìœ¼ë¡œ í¬í•¨

# 2. ì£¼ë§ íŠ¹ì„±
df_feat['is_weekend'] = df_feat['weekday'].apply(lambda x: 1 if x >= 5 else 0)

# 3. ê³„ì ˆ íŠ¹ì„± (ë¶ë°˜êµ¬ ê¸°ì¤€)
def get_season(doy):
    if 80 <= doy < 172: return 'Spring'
    elif 172 <= doy < 264: return 'Summer'
    elif 264 <= doy < 355: return 'Autumn'
    else: return 'Winter'
df_feat['season'] = df_feat['day_of_year'].apply(get_season)

# ë²”ì£¼í˜• ë³€ìˆ˜ ì›-í•« ì¸ì½”ë”©
df_feat = pd.get_dummies(df_feat, columns=['time_of_day', 'season'], drop_first=True)
print("âœ… [3-1ë‹¨ê³„] ì§€ëŠ¥í˜• íŠ¹ì„± ê³µí•™(ì‹œê°„ëŒ€, ì£¼ë§, ê³„ì ˆ) ë° ì›-í•« ì¸ì½”ë”© ì™„ë£Œ.")

# --- ê¸°ì¡´ ë¡¤ë§ íŠ¹ì„± ---
for window in [24, 48, 168, 336]:
    df_feat[f'OT_rollmean_{window}'] = df_feat['OT'].rolling(window, min_periods=1).mean().shift(1)
    df_feat[f'OT_rollstd_{window}'] = df_feat['OT'].rolling(window, min_periods=1).std().shift(1)
    df_feat[f'OT_lag_{window}'] = df_feat['OT'].shift(window)
    df_feat[f'OT_diff_{window}'] = df_feat['OT'].diff(periods=window)

df_feat.fillna(0, inplace=True)

# --- ìµœì¢… íŠ¹ì„± ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ---
# ê¸°ì¡´ íŠ¹ì„±ì—ì„œ ì›ë³¸ ë²”ì£¼í˜• ë³€ìˆ˜ ì œê±° ë° ì›-í•« ì¸ì½”ë”©ëœ ë³€ìˆ˜ ì¶”ê°€
base_feature_cols = ['HUFL','HULL','MUFL','MULL','LUFL','LULL','OT']
cyclical_cols = ['sin_hour','cos_hour','sin_wd','cos_wd', 'sin_doy', 'cos_doy']
rolling_cols = [col for col in df_feat.columns if 'OT_' in col]
# ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ìƒì„±ëœ ìƒˆë¡œìš´ íŠ¹ì„± ì´ë¦„ë“¤ì„ ë™ì ìœ¼ë¡œ ì¶”ê°€
ohe_cols = [col for col in df_feat.columns if 'time_of_day_' in col or 'season_' in col or 'is_weekend' in col]

feature_cols = base_feature_cols + cyclical_cols + ohe_cols + rolling_cols
print("âœ… [3-2ë‹¨ê³„] ì „ì²´ íŠ¹ì„± ê³µí•™ ì™„ë£Œ.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ë°ì´í„° ì •ê·œí™” (ëˆ„ìˆ˜ ë°©ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
train_end_date = "2018-01-01 00:00:00"
train_data = df_feat[df_feat.index < train_end_date]

scaler = MinMaxScaler()
scaler.fit(train_data[feature_cols])
ot_scaler = MinMaxScaler()
ot_scaler.fit(train_data[['OT']])

data_scaled = scaler.transform(df_feat[feature_cols])
scaled_df = pd.DataFrame(data_scaled, index=df_feat.index, columns=feature_cols)
print(f"âœ… [4ë‹¨ê³„] ëˆ„ìˆ˜ ë°©ì§€ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ. (í›ˆë ¨ ë°ì´í„° ì¢…ë£Œ ì‹œì : {train_end_date})")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ëª¨ë¸ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class MovingAvg(nn.Module):
    def __init__(self, kernel_size, stride):
        super().__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)
    def forward(self, x):
        front = x[:, 0:1, :].repeat(1,(self.kernel_size-1)//2,1)
        end = x[:,-1:,:].repeat(1,(self.kernel_size-1)//2,1)
        return self.avg(torch.cat([front,x,end],dim=1).permute(0,2,1)).permute(0,2,1)

class DLinear_Leakproof(nn.Module):
    def __init__(self, input_len, pred_len, kernel_size, dropout_rate, feature_dim, **kwargs):
        super().__init__()
        self.decompsition = MovingAvg(kernel_size, stride=1)
        # *** ì…ë ¥ ì°¨ì›ì„ feature_dimìœ¼ë¡œ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì • ***
        self.linear_seasonal = nn.Sequential(nn.Linear(input_len, pred_len), nn.Dropout(dropout_rate))
        self.linear_trend = nn.Sequential(nn.Linear(input_len, pred_len), nn.Dropout(dropout_rate))
    def forward(self, x):
        # OT íŠ¹ì„±ì˜ ì¸ë±ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ì°¾ê¸°
        ot_index = feature_cols.index('OT')
        x_ot = x[:, :, ot_index].unsqueeze(-1)
        trend = self.decompsition(x_ot)
        seasonal = (x_ot - trend).squeeze(-1); trend = trend.squeeze(-1)
        return self.linear_seasonal(seasonal) + self.linear_trend(trend)

class TS_Dataset(Dataset):
    def __init__(self, df_data, input_len, pred_len, feature_names):
        self.data = df_data.values
        self.il = input_len
        self.pl = pred_len
        self.feature_names = feature_names
    def __len__(self): return len(self.data)-self.il-self.pl+1
    def __getitem__(self, idx):
        x = self.data[idx:idx+self.il]
        # OT íŠ¹ì„±ì˜ ì¸ë±ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ì°¾ê¸°
        ot_index = self.feature_names.index('OT')
        y = self.data[idx+self.il:idx+self.il+self.pl, ot_index]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

print("âœ… [5ë‹¨ê³„] ëª¨ë¸ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. ì‹¤í—˜ ì„¤ì • (*** íŠ¹ì„± ê³µí•™ ì‹¤í—˜ 1ê°œ ì¶”ê°€ ***)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pred_len = 96
feature_dim = len(feature_cols) # íŠ¹ì„± ê°œìˆ˜ë¥¼ ë³€ìˆ˜ë¡œ ì €ì¥

experiment_configs = [
    # --- ê¸°ì¡´ ì‹¤í—˜ 12ê°œ ---
    {'exp_name': 'DLinear_len96', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_len168', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_len336_base', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_len504', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_len720', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_ks15', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_ks49', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_drop0.2', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_lr5e-5', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_best_combo', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_len504_MAE', 'criterion': nn.L1Loss()},
    {'exp_name': 'DLinear_best_combo_MAE', 'criterion': nn.L1Loss()},

    # --- (ì‹ ê·œ) íŠ¹ì„± ê³µí•™ ì‹¤í—˜ ---
    {'exp_name': 'DLinear_FeatEng_best_combo', 'criterion': nn.MSELoss()},
]

# ê¸°ë³¸ íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
default_params = {
    'DLinear_len96': {'input_len': 96, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_len168': {'input_len': 168, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_len336_base': {'input_len': 336, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_len504': {'input_len': 504, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_len720': {'input_len': 720, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_ks15': {'input_len': 336, 'kernel_size': 15, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_ks49': {'input_len': 336, 'kernel_size': 49, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_drop0.2': {'input_len': 336, 'kernel_size': 25, 'dropout_rate': 0.2, 'lr': 1e-4},
    'DLinear_lr5e-5': {'input_len': 336, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 5e-5},
    'DLinear_best_combo': {'input_len': 504, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 5e-5},
    'DLinear_len504_MAE': {'input_len': 504, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_best_combo_MAE': {'input_len': 504, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 5e-5},
    'DLinear_FeatEng_best_combo': {'input_len': 504, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 5e-5},
}

# ê° ì‹¤í—˜ ì„¤ì •ì— ê¸°ë³¸ íŒŒë¼ë¯¸í„° ë³‘í•©
for config in experiment_configs:
    config.update(default_params[config['exp_name']])

print(f"âœ… [6ë‹¨ê³„] ì´ {len(experiment_configs)}ê°œì˜ ì‹¤í—˜ ì„¤ì •ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. DLinear ëª¨ë¸ í›ˆë ¨, í‰ê°€, ì˜ˆì¸¡ í†µí•© ë£¨í”„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

all_results = {}

for config in experiment_configs:
    exp_name = config['exp_name']
    current_input_len = config['input_len']

    print(f"\n" + "="*80 + f"\nâ–¶ ì‹¤í—˜ [{exp_name}] ì‹œì‘\n" + "="*80)
    param_to_print = {k: v for k, v in config.items() if k not in ['criterion', 'exp_name']}
    print(f"   - íŒŒë¼ë¯¸í„°: {param_to_print}")
    print(f"   - ì†ì‹¤ í•¨ìˆ˜: {config['criterion'].__class__.__name__}")

    # --- ë°ì´í„°ë¡œë” ---
    train_df = scaled_df[scaled_df.index < train_end_date]
    # *** TS_Datasetì— feature_cols ì „ë‹¬ ***
    full_dataset = TS_Dataset(train_df, current_input_len, pred_len, feature_cols)
    val_split_idx = int(len(full_dataset) * 0.8)

    train_loader = DataLoader(Subset(full_dataset, range(val_split_idx)), batch_size=32, shuffle=True)
    val_loader = DataLoader(Subset(full_dataset, range(val_split_idx,len(full_dataset))), batch_size=32)

    # --- ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤ í•¨ìˆ˜ ---
    # *** ëª¨ë¸ ìƒì„± ì‹œ feature_dim ì „ë‹¬ ***
    model = DLinear_Leakproof(**config, pred_len=pred_len, feature_dim=feature_dim).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-5)
    criterion = config['criterion']
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)

    # --- í›ˆë ¨ ë£¨í”„ ---
    best_val_loss = float('inf')
    early_stop_counter = 0
    model_path = os.path.join(output_directory, f"model_{exp_name}.pth")

    for epoch in range(1, 201):
        model.train()
        train_losses = []
        for x,y in train_loader:
            x,y=x.to(device),y.to(device);optimizer.zero_grad();p=model(x);l=criterion(p,y);l.backward();torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0);optimizer.step()
            train_losses.append(l.item())
        avg_train_loss = np.mean(train_losses)

        model.eval(); val_losses=[]
        with torch.no_grad():
            for x,y in val_loader:x,y=x.to(device),y.to(device);p=model(x);val_losses.append(criterion(p,y).item())
        avg_val_loss = np.mean(val_losses)

        if (epoch-1) % 10 == 0 or epoch == 1:
            print(f"  [Ep {epoch:03d}] Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}")

        if avg_val_loss < best_val_loss:
            best_val_loss, early_stop_counter = avg_val_loss, 0
            torch.save(model.state_dict(), model_path)
        else:
            early_stop_counter += 1

        if early_stop_counter >= 30:
            print(f"  ì¡°ê¸° ì¢…ë£Œ. (Epoch {epoch})")
            break
        scheduler.step()

    print(f"  ìµœì  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path} (Best Val Loss: {best_val_loss:.6f})")

    # --- ì˜ˆì¸¡ íŒŒì¼ ìƒì„± ---
    print(f"\nâ–¶ ì˜ˆì¸¡ íŒŒì¼ ìƒì„± ì¤‘...")
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    submit_df_raw = pd.read_csv(sample_submission_path)
    results = []
    for _, row in submit_df_raw.iterrows():
        t0=pd.to_datetime(row[submit_df_raw.columns[0]]); input_end,input_start=t0-timedelta(hours=1),t0-timedelta(hours=current_input_len)
        seq=scaled_df.loc[input_start:input_end].values
        x_tensor=torch.tensor(seq,dtype=torch.float32).unsqueeze(0).to(device)
        with torch.no_grad(): pred_scaled=model(x_tensor).cpu().numpy().flatten()
        pred_unnorm = ot_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()
        results.append([row[submit_df_raw.columns[0]]] + pred_unnorm.tolist())

    col_names=[submit_df_raw.columns[0]] + [f"T{i}" for i in range(pred_len)]; df_submit=pd.DataFrame(results,columns=col_names)
    submission_filename = os.path.join(output_directory, f"submission_{exp_name}.csv")
    df_submit.to_csv(submission_filename, index=False)
    print(f"âœ… ì˜ˆì¸¡ íŒŒì¼ ìƒì„± ì™„ë£Œ: {submission_filename}")

    # --- ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™” ---
    print(f"\nâ–¶ ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™” ì¤‘...")
    y_pred_flat = df_submit.drop(columns=[submit_df_raw.columns[0]]).values.flatten()
    submit_times = pd.to_datetime(df_submit[submit_df_raw.columns[0]].values)
    gt_ots = []
    for t in submit_times:
        gt_range = pd.date_range(start=t, periods=pred_len, freq='H')
        gt_series = df.reindex(gt_range)['OT']
        gt_ots.append(gt_series.values)
    gt_ots_flat = np.concatenate(gt_ots)
    valid_indices = ~np.isnan(gt_ots_flat)
    gt_ots_clean = gt_ots_flat[valid_indices]
    y_pred_clean = y_pred_flat[valid_indices]

    def print_metrics(y_true, y_pred):
        mse = mean_squared_error(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred)
        print(f"  ã€ ìµœì¢… ì„±ëŠ¥ ì§€í‘œ ã€‘")
        print(f"  - MSE  : {mse:.4f} (Kaggle í‰ê°€ì§€í‘œ)")
        print(f"  - MAE  : {mae:.4f}")
        print(f"  - RMSE : {rmse:.4f}")
        print(f"  - RÂ²   : {r2:.4f}")
        return {'MSE': mse, 'MAE': mae, 'RMSE': rmse, 'R2': r2}

    metrics = print_metrics(gt_ots_clean, y_pred_clean)
    result_log = config.copy()
    result_log['criterion'] = config['criterion'].__class__.__name__
    result_log.update(metrics)
    all_results[exp_name] = result_log


    plt.figure(figsize=(12, 7))
    plt.hist(gt_ots_clean, bins=50, alpha=0.7, label='Ground Truth', color='royalblue')
    plt.hist(y_pred_clean, bins=50, alpha=0.7, label='Prediction', color='orangered')
    plt.title(f"Prediction vs. Ground Truth Distribution ({exp_name})", fontsize=16)
    plt.xlabel("OT Value", fontsize=12); plt.ylabel("Frequency", fontsize=12)
    plt.legend(); plt.grid(axis='y', linestyle='--', alpha=0.7); plt.tight_layout()
    hist_filename = os.path.join(output_directory, f"hist_{exp_name}.png")
    plt.savefig(hist_filename)
    plt.close()
    print(f"âœ… ë¹„êµ íˆìŠ¤í† ê·¸ë¨ ì €ì¥ ì™„ë£Œ: {hist_filename}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8. ìµœì¢… ê²°ê³¼ ìš”ì•½
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*80 + "\nâ–¶ ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ìš”ì•½\n" + "="*80)
results_df = pd.DataFrame.from_dict(all_results, orient='index')
results_df.index.name = 'Experiment Name'

# ë³´ê¸° ì¢‹ê²Œ ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
ordered_cols = ['input_len', 'kernel_size', 'dropout_rate', 'lr', 'criterion', 'val_loss', 'MSE', 'MAE', 'RMSE', 'R2']
final_cols = [col for col in ordered_cols if col in results_df.columns]
results_df = results_df[final_cols]
results_df = results_df.sort_values(by='MSE')

# ì†Œìˆ˜ì  ì •ë¦¬
pd.set_option('display.float_format', '{:.4f}'.format)

print(results_df)

best_exp = results_df.index[0]
best_mse = results_df.iloc[0]['MSE']
print(f"\nğŸ† ìµœì  ì‹¤í—˜: [{best_exp}] (Test MSE: {best_mse:.4f})")
print("âœ… ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")