# ==============================================================================
# [STEP 1] DLinear ì „ì²´ ì‹¤í—˜ ë£¨í”„ (ëª¨ë¸ í›ˆë ¨/ì˜ˆì¸¡/ê²°ê³¼ ì €ì¥: dlinear_all_experiments_results/)
# ==============================================================================
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from datetime import datetime, timedelta
import math
import os
import matplotlib.pyplot as plt

# ì‹œë“œ ê³ ì •
torch.manual_seed(42)
np.random.seed(42)
print("âœ… [1ë‹¨ê³„] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ.")

# ê²½ë¡œ ì„¤ì •
BASE_PATH = os.getcwd()
data_path = os.path.join(BASE_PATH, "ETTh1.csv")
sample_submission_path = os.path.join(BASE_PATH, "sample_submit.csv")
dlinear_output_directory = os.path.join(BASE_PATH, "dlinear_all_experiments_results/")
os.makedirs(dlinear_output_directory, exist_ok=True)
print(f"ğŸ“‚ ëª¨ë“  ì¶œë ¥ íŒŒì¼ì€ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥ë©ë‹ˆë‹¤: {dlinear_output_directory}")

# ë°ì´í„° + íŠ¹ì„±ê³µí•™
df = pd.read_csv(data_path)
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)
df_feat = df.copy()
df_feat['hour'] = df_feat.index.hour
df_feat['weekday'] = df_feat.index.weekday
df_feat['day_of_year'] = df_feat.index.dayofyear
def cyclical_encode(vals, max_val):
    radians = 2.0 * math.pi * vals / max_val
    return np.sin(radians), np.cos(radians)
sin_h, cos_h = cyclical_encode(df_feat['hour'].values, 24)
sin_wd, cos_wd = cyclical_encode(df_feat['weekday'].values, 7)
sin_doy, cos_doy = cyclical_encode(df_feat['day_of_year'].values, 366)
df_feat['sin_hour'], df_feat['cos_hour'] = sin_h, cos_h
df_feat['sin_wd'], df_feat['cos_wd'] = sin_wd, cos_wd
df_feat['sin_doy'], df_feat['cos_doy'] = sin_doy, cos_doy
bins = [0, 6, 12, 18, 23]; labels = ['Night', 'Morning', 'Afternoon', 'Evening']
df_feat['time_of_day'] = pd.cut(df_feat['hour'], bins=bins, labels=labels, right=False, ordered=False)
df_feat.loc[df_feat['hour'] == 23, 'time_of_day'] = 'Evening'
df_feat['is_weekend'] = df_feat['weekday'].apply(lambda x: 1 if x >= 5 else 0)
def get_season(doy):
    if 80 <= doy < 172: return 'Spring'
    elif 172 <= doy < 264: return 'Summer'
    elif 264 <= doy < 355: return 'Autumn'
    else: return 'Winter'
df_feat['season'] = df_feat['day_of_year'].apply(get_season)
df_feat = pd.get_dummies(df_feat, columns=['time_of_day', 'season'], drop_first=True)
for window in [24, 48, 168, 336]:
    df_feat[f'OT_rollmean_{window}'] = df_feat['OT'].rolling(window, min_periods=1).mean().shift(1)
    df_feat[f'OT_rollstd_{window}'] = df_feat['OT'].rolling(window, min_periods=1).std().shift(1)
    df_feat[f'OT_lag_{window}'] = df_feat['OT'].shift(window)
    df_feat[f'OT_diff_{window}'] = df_feat['OT'].diff(periods=window)
df_feat.fillna(0, inplace=True)
base_feature_cols = ['HUFL','HULL','MUFL','MULL','LUFL','LULL','OT']
cyclical_cols = ['sin_hour','cos_hour','sin_wd','cos_wd', 'sin_doy', 'cos_doy']
rolling_cols = [col for col in df_feat.columns if 'OT_' in col]
ohe_cols = [col for col in df_feat.columns if 'time_of_day_' in col or 'season_' in col or 'is_weekend' in col]
feature_cols = base_feature_cols + cyclical_cols + ohe_cols + rolling_cols
print("âœ… [3-2ë‹¨ê³„] ì „ì²´ íŠ¹ì„± ê³µí•™ ì™„ë£Œ.")

# ì •ê·œí™”
train_end_date = "2018-01-01 00:00:00"
train_data = df_feat[df_feat.index < train_end_date]
scaler = MinMaxScaler()
scaler.fit(train_data[feature_cols])
ot_scaler = MinMaxScaler()
ot_scaler.fit(train_data[['OT']])
data_scaled = scaler.transform(df_feat[feature_cols])
scaled_df = pd.DataFrame(data_scaled, index=df_feat.index, columns=feature_cols)
print(f"âœ… [4ë‹¨ê³„] ëˆ„ìˆ˜ ë°©ì§€ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ. (í›ˆë ¨ ë°ì´í„° ì¢…ë£Œ ì‹œì : {train_end_date})")

# DLinear ëª¨ë¸/ë°ì´í„°ì…‹ ì •ì˜ (ë™ì¼)
class MovingAvg(nn.Module):
    def __init__(self, kernel_size, stride):
        super().__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)
    def forward(self, x):
        front = x[:, 0:1, :].repeat(1,(self.kernel_size-1)//2,1)
        end = x[:,-1:,:].repeat(1,(self.kernel_size-1)//2,1)
        return self.avg(torch.cat([front,x,end],dim=1).permute(0,2,1)).permute(0,2,1)
class DLinear_Leakproof(nn.Module):
    def __init__(self, input_len, pred_len, kernel_size, dropout_rate, feature_dim, **kwargs):
        super().__init__()
        self.decompsition = MovingAvg(kernel_size, stride=1)
        self.linear_seasonal = nn.Sequential(nn.Linear(input_len, pred_len), nn.Dropout(dropout_rate))
        self.linear_trend = nn.Sequential(nn.Linear(input_len, pred_len), nn.Dropout(dropout_rate))
    def forward(self, x):
        ot_index = feature_cols.index('OT')
        x_ot = x[:, :, ot_index].unsqueeze(-1)
        trend = self.decompsition(x_ot)
        seasonal = (x_ot - trend).squeeze(-1); trend = trend.squeeze(-1)
        return self.linear_seasonal(seasonal) + self.linear_trend(trend)
class TS_Dataset(Dataset):
    def __init__(self, df_data, input_len, pred_len, feature_names):
        self.data = df_data.values
        self.il = input_len
        self.pl = pred_len
        self.feature_names = feature_names
    def __len__(self): return len(self.data)-self.il-self.pl+1
    def __getitem__(self, idx):
        x = self.data[idx:idx+self.il]
        ot_index = self.feature_names.index('OT')
        y = self.data[idx+self.il:idx+self.il+self.pl, ot_index]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)
print("âœ… [5ë‹¨ê³„] ëª¨ë¸ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ.")

# ì‹¤í—˜ì„¤ì •
pred_len = 96
feature_dim = len(feature_cols)
experiment_configs = [
    {'exp_name': 'DLinear_len96', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_len168', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_len336_base', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_len504', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_len720', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_ks15', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_ks49', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_drop0.2', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_lr5e-5', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_best_combo', 'criterion': nn.MSELoss()},
    {'exp_name': 'DLinear_len504_MAE', 'criterion': nn.L1Loss()},
    {'exp_name': 'DLinear_best_combo_MAE', 'criterion': nn.L1Loss()},
    {'exp_name': 'DLinear_FeatEng_best_combo', 'criterion': nn.MSELoss()},
]
default_params = {
    'DLinear_len96': {'input_len': 96, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_len168': {'input_len': 168, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_len336_base': {'input_len': 336, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_len504': {'input_len': 504, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_len720': {'input_len': 720, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_ks15': {'input_len': 336, 'kernel_size': 15, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_ks49': {'input_len': 336, 'kernel_size': 49, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_drop0.2': {'input_len': 336, 'kernel_size': 25, 'dropout_rate': 0.2, 'lr': 1e-4},
    'DLinear_lr5e-5': {'input_len': 336, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 5e-5},
    'DLinear_best_combo': {'input_len': 504, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 5e-5},
    'DLinear_len504_MAE': {'input_len': 504, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 1e-4},
    'DLinear_best_combo_MAE': {'input_len': 504, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 5e-5},
    'DLinear_FeatEng_best_combo': {'input_len': 504, 'kernel_size': 25, 'dropout_rate': 0.1, 'lr': 5e-5},
}
for config in experiment_configs:
    config.update(default_params[config['exp_name']])
print(f"âœ… [6ë‹¨ê³„] ì´ {len(experiment_configs)}ê°œì˜ ì‹¤í—˜ ì„¤ì •ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")

# === í›ˆë ¨/ì˜ˆì¸¡ ë£¨í”„ ===
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
all_results = {}
for config in experiment_configs:
    exp_name = config['exp_name']
    current_input_len = config['input_len']
    print(f"\n" + "="*80 + f"\nâ–¶ ì‹¤í—˜ [{exp_name}] ì‹œì‘\n" + "="*80)
    train_df = scaled_df[scaled_df.index < train_end_date]
    full_dataset = TS_Dataset(train_df, current_input_len, pred_len, feature_cols)
    val_split_idx = int(len(full_dataset) * 0.8)
    train_loader = DataLoader(Subset(full_dataset, range(val_split_idx)), batch_size=32, shuffle=True)
    val_loader = DataLoader(Subset(full_dataset, range(val_split_idx,len(full_dataset))), batch_size=32)
    model = DLinear_Leakproof(**config, pred_len=pred_len, feature_dim=feature_dim).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-5)
    criterion = config['criterion']
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)
    best_val_loss = float('inf')
    early_stop_counter = 0
    model_path = os.path.join(dlinear_output_directory, f"model_{exp_name}.pth")
    for epoch in range(1, 201):
        model.train()
        train_losses = []
        for x,y in train_loader:
            x,y=x.to(device),y.to(device);optimizer.zero_grad();p=model(x);l=criterion(p,y);l.backward();torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0);optimizer.step()
            train_losses.append(l.item())
        avg_train_loss = np.mean(train_losses)
        model.eval(); val_losses=[]
        with torch.no_grad():
            for x,y in val_loader:x,y=x.to(device),y.to(device);p=model(x);val_losses.append(criterion(p,y).item())
        avg_val_loss = np.mean(val_losses)
        if (epoch-1) % 10 == 0 or epoch == 1:
            print(f"  [Ep {epoch:03d}] Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}")
        if avg_val_loss < best_val_loss:
            best_val_loss, early_stop_counter = avg_val_loss, 0
            torch.save(model.state_dict(), model_path)
        else:
            early_stop_counter += 1
        if early_stop_counter >= 30:
            print(f"  ì¡°ê¸° ì¢…ë£Œ. (Epoch {epoch})")
            break
        scheduler.step()
    print(f"  ìµœì  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path} (Best Val Loss: {best_val_loss:.6f})")
    print(f"\nâ–¶ ì˜ˆì¸¡ íŒŒì¼ ìƒì„± ì¤‘...")
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    submit_df_raw = pd.read_csv(sample_submission_path)
    results = []
    for _, row in submit_df_raw.iterrows():
        t0=pd.to_datetime(row[submit_df_raw.columns[0]]); input_end,input_start=t0-timedelta(hours=1),t0-timedelta(hours=current_input_len)
        seq=scaled_df.loc[input_start:input_end].values
        x_tensor=torch.tensor(seq,dtype=torch.float32).unsqueeze(0).to(device)
        with torch.no_grad(): pred_scaled=model(x_tensor).cpu().numpy().flatten()
        pred_unnorm = ot_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()
        results.append([row[submit_df_raw.columns[0]]] + pred_unnorm.tolist())
    col_names=[submit_df_raw.columns[0]] + [f"T{i}" for i in range(pred_len)]; df_submit=pd.DataFrame(results,columns=col_names)
    submission_filename = os.path.join(dlinear_output_directory, f"submission_{exp_name}.csv")
    df_submit.to_csv(submission_filename, index=False)
    print(f"âœ… ì˜ˆì¸¡ íŒŒì¼ ìƒì„± ì™„ë£Œ: {submission_filename}")

# ==============================================================================
# [STEP 2] ì¡°ê±´ë¶€ ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„± (dlinear_ensemble_results/)
# ==============================================================================

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì‹œë“œ ê³ ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from datetime import datetime, timedelta
import math
import os
import matplotlib.pyplot as plt

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
torch.manual_seed(42)
np.random.seed(42)

print("âœ… [1ë‹¨ê³„] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ.")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ê²½ë¡œ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
BASE_PATH = os.getcwd()

# === ì¤‘ìš”: ë¯¸ë¦¬ í›ˆë ¨ëœ ëª¨ë¸ íŒŒì¼(.pth)ë“¤ì´ ì €ì¥ëœ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš” ===
# ì˜ˆì‹œ: ì´ì „ dl3.pyë¥¼ ì‹¤í–‰í•œ ê²°ê³¼ í´ë”
PRE_TRAINED_MODEL_PATH = os.path.join(BASE_PATH, "dlinear_all_experiments_results/")

# ì…ë ¥ ë°ì´í„° ê²½ë¡œ
ETTH1_PATH = os.path.join(BASE_PATH, "ETTh1.csv")
SUBMIT_SAMPLE_PATH = os.path.join(BASE_PATH, "sample_submit.csv")

# ìµœì¢… ê²°ê³¼ë¬¼ì´ ì €ì¥ë  ë””ë ‰í† ë¦¬
output_directory = os.path.join(BASE_PATH, "conditional_ensemble_result/")
os.makedirs(output_directory, exist_ok=True)
print(f"ğŸ“‚ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ê²½ë¡œ: {PRE_TRAINED_MODEL_PATH}")
print(f"ğŸ“‚ ìµœì¢… ê²°ê³¼ë¬¼ ì €ì¥ ê²½ë¡œ: {output_directory}")


#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
try:
    df = pd.read_csv(ETTH1_PATH)
    submit_df_raw = pd.read_csv(SUBMIT_SAMPLE_PATH)
except FileNotFoundError as e:
    print(f"âŒ íŒŒì¼ ì˜¤ë¥˜: {e}")
    print("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— ETTh1.csvì™€ sample_submit.csv íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit()

df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

# ëª¨ë¸ í›ˆë ¨ ì‹œì™€ ë™ì¼í•œ íŠ¹ì„± ê³µí•™ ìˆ˜í–‰
df_feat = df.copy()
df_feat['hour'] = df_feat.index.hour
df_feat['weekday'] = df_feat.index.weekday
df_feat['day_of_year'] = df_feat.index.dayofyear

def cyclical_encode(vals, max_val):
    radians = 2.0 * math.pi * vals / max_val
    return np.sin(radians), np.cos(radians)

sin_h, cos_h = cyclical_encode(df_feat['hour'].values, 24)
sin_wd, cos_wd = cyclical_encode(df_feat['weekday'].values, 7)
sin_doy, cos_doy = cyclical_encode(df_feat['day_of_year'].values, 366)
df_feat['sin_hour'], df_feat['cos_hour'] = sin_h, cos_h
df_feat['sin_wd'], df_feat['cos_wd'] = sin_wd, cos_wd
df_feat['sin_doy'], df_feat['cos_doy'] = sin_doy, cos_doy

bins = [0, 6, 12, 18, 23]; labels = ['Night', 'Morning', 'Afternoon', 'Evening']
df_feat['time_of_day'] = pd.cut(df_feat['hour'], bins=bins, labels=labels, right=False, ordered=False)
df_feat.loc[df_feat['hour'] == 23, 'time_of_day'] = 'Evening'
df_feat['is_weekend'] = df_feat['weekday'].apply(lambda x: 1 if x >= 5 else 0)

def get_season(doy):
    if 80 <= doy < 172: return 'Spring'
    elif 172 <= doy < 264: return 'Summer'
    elif 264 <= doy < 355: return 'Autumn'
    else: return 'Winter'
df_feat['season'] = df_feat['day_of_year'].apply(get_season)
df_feat = pd.get_dummies(df_feat, columns=['time_of_day', 'season'], drop_first=True)

for window in [24, 48, 168, 336]:
    df_feat[f'OT_rollmean_{window}'] = df_feat['OT'].rolling(window, min_periods=1).mean().shift(1)
    df_feat[f'OT_rollstd_{window}'] = df_feat['OT'].rolling(window, min_periods=1).std().shift(1)
    df_feat[f'OT_lag_{window}'] = df_feat['OT'].shift(window)
    df_feat[f'OT_diff_{window}'] = df_feat['OT'].diff(periods=window)

df_feat.fillna(0, inplace=True)

base_feature_cols = ['HUFL','HULL','MUFL','MULL','LUFL','LULL','OT']
cyclical_cols = ['sin_hour','cos_hour','sin_wd','cos_wd', 'sin_doy', 'cos_doy']
rolling_cols = [col for col in df_feat.columns if 'OT_' in col]
ohe_cols = [col for col in df_feat.columns if 'time_of_day_' in col or 'season_' in col or 'is_weekend' in col]
feature_cols = base_feature_cols + cyclical_cols + ohe_cols + rolling_cols

# ë°ì´í„° ì •ê·œí™”
train_end_date = "2018-01-01 00:00:00"
train_data = df_feat[df_feat.index < train_end_date]
scaler = MinMaxScaler()
scaler.fit(train_data[feature_cols])
ot_scaler = MinMaxScaler()
ot_scaler.fit(train_data[['OT']])
data_scaled = scaler.transform(df_feat[feature_cols])
scaled_df = pd.DataFrame(data_scaled, index=df_feat.index, columns=feature_cols)

print("âœ… [3ë‹¨ê³„] ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ.")


#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ë° ëª¨ë¸ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

class MovingAvg(nn.Module):
    def __init__(self, kernel_size, stride):
        super().__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)
    def forward(self, x):
        front = x[:, 0:1, :].repeat(1,(self.kernel_size-1)//2,1)
        end = x[:,-1:,:].repeat(1,(self.kernel_size-1)//2,1)
        return self.avg(torch.cat([front,x,end],dim=1).permute(0,2,1)).permute(0,2,1)

class DLinear_Leakproof(nn.Module):
    def __init__(self, input_len, pred_len, kernel_size, dropout_rate):
        super().__init__()
        self.decompsition = MovingAvg(kernel_size, stride=1)
        self.linear_seasonal = nn.Sequential(nn.Linear(input_len, pred_len), nn.Dropout(dropout_rate))
        self.linear_trend = nn.Sequential(nn.Linear(input_len, pred_len), nn.Dropout(dropout_rate))
    def forward(self, x):
        ot_index = feature_cols.index('OT')
        x_ot = x[:, :, ot_index].unsqueeze(-1)
        trend = self.decompsition(x_ot)
        seasonal = (x_ot - trend).squeeze(-1); trend = trend.squeeze(-1)
        return self.linear_seasonal(seasonal) + self.linear_trend(trend)

# ì•™ìƒë¸”í•  'ì§€ì—­ ì „ë¬¸ê°€' ëª¨ë¸ë“¤
pred_len = 96
ensemble_members = {
    'high_temp_expert': {
        'exp_name': 'DLinear_best_combo',
        'params': {'input_len': 504, 'pred_len': pred_len, 'kernel_size': 25, 'dropout_rate': 0.1},
    },
    'low_temp_expert': {
        'exp_name': 'DLinear_best_combo_MAE',
        'params': {'input_len': 504, 'pred_len': pred_len, 'kernel_size': 25, 'dropout_rate': 0.1},
    },
    'guide': {
        'exp_name': 'DLinear_len336_base',
        'params': {'input_len': 336, 'pred_len': pred_len, 'kernel_size': 25, 'dropout_rate': 0.1},
    }
}

models = {}
try:
    for role, config in ensemble_members.items():
        model = DLinear_Leakproof(**config['params']).to(device)
        model_path = os.path.join(PRE_TRAINED_MODEL_PATH, f"model_{config['exp_name']}.pth")
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()
        models[role] = model
        print(f"  - [{role.replace('_', ' ').title()}] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
except FileNotFoundError as e:
    print(f"âŒ ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print(f"'{PRE_TRAINED_MODEL_PATH}' ê²½ë¡œì— í›ˆë ¨ëœ ëª¨ë¸ íŒŒì¼(.pth)ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit()

print("âœ… [4ë‹¨ê³„] ëª¨ë“  ì „ë¬¸ê°€ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")


#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ì¡°ê±´ë¶€ ì•™ìƒë¸” ìˆ˜í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
print("\n" + "="*80 + "\nâ–¶ PART 5: ì¡°ê±´ë¶€ ì•™ìƒë¸” ìƒì„± ì‹œì‘\n" + "="*80)

SWITCHING_THRESHOLD = 7.5
HIGH_TEMP_WEIGHTS = {'high_temp_expert': 0.7, 'low_temp_expert': 0.2, 'guide': 0.1}
LOW_TEMP_WEIGHTS = {'high_temp_expert': 0.2, 'low_temp_expert': 0.7, 'guide': 0.1}

ensemble_results = []
for _, row in submit_df_raw.iterrows():
    t0 = pd.to_datetime(row[submit_df_raw.columns[0]])
    predictions = {}

    for role, model in models.items():
        input_len = ensemble_members[role]['params']['input_len']
        input_start = t0 - timedelta(hours=input_len); input_end = t0 - timedelta(hours=1)
        seq = scaled_df.loc[input_start:input_end].values
        x_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0).to(device)
        with torch.no_grad():
            pred_scaled = model(x_tensor).cpu().numpy().flatten()
        predictions[role] = ot_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()

    guide_preds = predictions['guide']
    final_pred = np.zeros(pred_len)
    for i in range(pred_len):
        weights = HIGH_TEMP_WEIGHTS if guide_preds[i] >= SWITCHING_THRESHOLD else LOW_TEMP_WEIGHTS
        final_pred[i] = sum(predictions[role][i] * weights[role] for role in weights)
    ensemble_results.append([row[0]] + final_pred.tolist())

col_names=[submit_df_raw.columns[0]] + [f"T{i}" for i in range(pred_len)]
conditional_ensemble_df = pd.DataFrame(ensemble_results, columns=col_names)

print("âœ… [5ë‹¨ê³„] ì¡°ê±´ë¶€ ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ìƒì„± ì™„ë£Œ.")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. ìµœì¢… ê²°ê³¼ ì €ì¥ ë° í‰ê°€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
print("\n" + "="*80 + "\nâ–¶ PART 6: ìµœì¢… ê²°ê³¼ ì €ì¥ ë° í‰ê°€ ì‹œì‘\n" + "="*80)

submission_filename = os.path.join(output_directory, "submission_conditional_ensemble.csv")
conditional_ensemble_df.to_csv(submission_filename, index=False)
print(f"âœ… ì¡°ê±´ë¶€ ì•™ìƒë¸” ì˜ˆì¸¡ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {submission_filename}")

# --- ì„±ëŠ¥ í‰ê°€ ---
y_pred_flat = conditional_ensemble_df.drop(columns=[submit_df_raw.columns[0]]).values.flatten()
submit_times = pd.to_datetime(conditional_ensemble_df[submit_df_raw.columns[0]].values)
gt_ots = []
for t in submit_times:
    gt_range = pd.date_range(start=t, periods=pred_len, freq='H')
    gt_series = df.reindex(gt_range)['OT']
    gt_ots.append(gt_series.values)
gt_ots_flat = np.concatenate(gt_ots)
valid_indices = ~np.isnan(gt_ots_flat)
gt_ots_clean = gt_ots_flat[valid_indices]
y_pred_clean = y_pred_flat[valid_indices]

mse = mean_squared_error(gt_ots_clean, y_pred_clean)
mae = mean_absolute_error(gt_ots_clean, y_pred_clean)
rmse = np.sqrt(mse)
r2 = r2_score(gt_ots_clean, y_pred_clean)

print("\n  ã€ ì¡°ê±´ë¶€ ì•™ìƒë¸” ìµœì¢… ì„±ëŠ¥ ì§€í‘œ ã€‘")
print(f"  - MSE  : {mse:.4f} (Kaggle í‰ê°€ì§€í‘œ)")
print(f"  - MAE  : {mae:.4f}")
print(f"  - RMSE : {rmse:.4f}")
print(f"  - RÂ²   : {r2:.4f}")

# --- ì‹œê°í™” ---
plt.figure(figsize=(12, 7))
plt.hist(gt_ots_clean, bins=50, alpha=0.7, label='Ground Truth', color='royalblue')
plt.hist(y_pred_clean, bins=50, alpha=0.7, label='Conditional Ensemble Prediction', color='green')
plt.title("Conditional Ensemble Prediction vs. Ground Truth Distribution", fontsize=16)
plt.xlabel("OT Value", fontsize=12); plt.ylabel("Frequency", fontsize=12)
plt.legend(); plt.grid(axis='y', linestyle='--', alpha=0.7); plt.tight_layout()
hist_filename = os.path.join(output_directory, "hist_conditional_ensemble.png")
plt.savefig(hist_filename)
plt.close()
print(f"\nâœ… ë¹„êµ íˆìŠ¤í† ê·¸ë¨ ì €ì¥ ì™„ë£Œ: {hist_filename}")

print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ‰")

# ==============================================================================
# [STEP 3] Transformer ë‹¨ë… ì‹¤í–‰ (transformer_results/)
# ==============================================================================
#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ë° ëœë¤ ì‹œë“œ ê³ ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime, timedelta
import math
import os
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

torch.manual_seed(42)
np.random.seed(42)

print("âœ… [1ë‹¨ê³„] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì‹œë“œ ê³ ì • ì™„ë£Œ.")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ê²½ë¡œ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# ëª¨ë“  íŒŒì¼ì´ ì €ì¥ë˜ê³  ë¡œë“œë  ê¸°ë³¸ ê²½ë¡œ
BASE_PATH = os.getcwd()
output_directory = os.path.join(BASE_PATH, "transformer_results/")
os.makedirs(output_directory, exist_ok=True)
print(f"ğŸ“‚ ìµœì¢… ê²°ê³¼ë¬¼ì€ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥ë©ë‹ˆë‹¤: {output_directory}")


# ì…ë ¥ íŒŒì¼ ê²½ë¡œ ì •ì˜
ETTH1_PATH = os.path.join(BASE_PATH, "ETTh1.csv")
SUBMIT_SAMPLE_PATH = os.path.join(BASE_PATH, "sample_submit.csv")

# ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì •ì˜
MODEL_SAVE_PATH = os.path.join(output_directory, "best_transformer_model.pth")
SUBMISSION_CSV_PATH = os.path.join(output_directory, "submission_transformer.csv")
HIST_IMG_PATH = os.path.join(output_directory, "histogram_transformer.png")

print("âœ… [2ë‹¨ê³„] ëª¨ë“  íŒŒì¼ ê²½ë¡œ ì„¤ì • ì™„ë£Œ.")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ë°ì´í„° ë¡œë“œ ë° íŠ¹ì„± ê³µí•™
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
try:
    # ì›ë³¸ ë°ì´í„°ëŠ” 'df' ë¼ëŠ” ë³€ìˆ˜ëª…ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
    df = pd.read_csv(ETTH1_PATH)
    submit_df_raw = pd.read_csv(SUBMIT_SAMPLE_PATH)
except FileNotFoundError as e:
    print(f"âŒ íŒŒì¼ ì˜¤ë¥˜: {e}")
    print("ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— ETTh1.csvì™€ sample_submit.csv íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit()

df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

df_feat = df.copy()
df_feat['hour']    = df_feat.index.hour
df_feat['weekday'] = df_feat.index.weekday

def cyclical_encode(vals, max_val):
    radians = 2.0 * math.pi * vals / max_val
    return np.sin(radians), np.cos(radians)

sin_h, cos_h   = cyclical_encode(df_feat['hour'].values, 24)
sin_wd, cos_wd = cyclical_encode(df_feat['weekday'].values, 7)
df_feat['sin_hour'], df_feat['cos_hour'] = sin_h, cos_h
df_feat['sin_wd'], df_feat['cos_wd']     = sin_wd, cos_wd

for window in [24, 48, 168]:
    df_feat[f'OT_rollmean_{window}'] = df_feat['OT'].rolling(window).mean().bfill()
    df_feat[f'OT_rollstd_{window}']  = df_feat['OT'].rolling(window).std().bfill()
    df_feat[f'OT_lag_{window}']      = df_feat['OT'].shift(window).bfill()
    df_feat[f'OT_diff_{window}']     = df_feat['OT'] - df_feat['OT'].shift(window).bfill()

feature_cols = [
    'HUFL','HULL','MUFL','MULL','LUFL','LULL','OT',
    'sin_hour','cos_hour','sin_wd','cos_wd',
    'OT_rollmean_24','OT_rollmean_48','OT_rollmean_168',
    'OT_rollstd_24','OT_rollstd_48','OT_rollstd_168',
    'OT_lag_24','OT_lag_48','OT_lag_168',
    'OT_diff_24','OT_diff_48','OT_diff_168',
]
# ìƒì„±ëœ íŠ¹ì„±ë§Œ ì‚¬ìš©í•˜ë„ë¡ DataFrame í•„í„°ë§
df_feat = df_feat[feature_cols]

print("âœ… [3ë‹¨ê³„] íŠ¹ì„± ê³µí•™ ì™„ë£Œ.")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ë°ì´í„° ì •ê·œí™” ë° ë¶„í• 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(df_feat)
scaled_df = pd.DataFrame(data_scaled, index=df_feat.index, columns=feature_cols)

# ìŠ¤ì¼€ì¼ëœ OTì˜ ë¶„ìœ„ìˆ˜ ê¸°ì¤€ ì„ê³„ê°’ ê³„ì‚°
low_thr_scaled, high_thr_scaled = np.percentile(scaled_df['OT'], [20, 80])
print(f"ğŸ“Š ê°€ì¤‘ ì†ì‹¤(WeightedLoss) ì„ê³„ê°’: 20%={low_thr_scaled:.3f} | 80%={high_thr_scaled:.3f}")

# í•™ìŠµ/ê²€ì¦ ë¶„í•  ê¸°ì¤€ ì„¤ì • (Leak ë°©ì§€)
last_label_date = datetime(2018,1,31,23,0)
max_t0_date     = last_label_date - timedelta(hours=95)
df_trainval = scaled_df[:last_label_date]

input_len = 172
pred_len  = 96

print("âœ… [4ë‹¨ê³„] ë°ì´í„° ì •ê·œí™” ë° ë¶„í•  ì™„ë£Œ.")


#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ëª¨ë¸ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

class TS_Dataset(Dataset):
    def __init__(self, df, input_len, pred_len, feature_cols, max_t0_date, last_label_date):
        self.feat = df[feature_cols].values
        self.input_len, self.pred_len = input_len, pred_len
        self.feature_cols = feature_cols
        last_label_idx = df.index.get_loc(last_label_date)
        max_t0_idx = df.index.get_loc(max_t0_date)
        self.start_idx = [i for i in range(self.input_len, max_t0_idx + 1) if (i + self.pred_len - 1) <= last_label_idx]
    def __len__(self): return len(self.start_idx)
    def __getitem__(self, idx):
        i = self.start_idx[idx]
        x = self.feat[i - self.input_len : i]
        y = self.feat[i : i + self.pred_len, self.feature_cols.index('OT')]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

def mark_high_windows(start_idx_list, feat_array, input_len, high_cut):
    flags = []
    for i in start_idx_list:
        win = feat_array[i - input_len : i, feature_cols.index('OT')]
        flags.append( (win >= high_cut).any() )
    return np.array(flags)

class AsymWeightedMSE(nn.Module):
    def __init__(self, low_thr, high_thr, w_low=0.2, w_high=2.0):
        super().__init__()
        self.low_thr, self.high_thr = low_thr, high_thr
        self.w_low, self.w_high = w_low, w_high
    def forward(self, pred, target):
        w = torch.ones_like(target)
        w[target < self.low_thr] = self.w_low
        w[target >= self.high_thr] = self.w_high
        return (w * (pred - target) ** 2).mean()

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))
    def forward(self, x):
        return x + self.pe[:, :x.size(1), :].to(x.device)

class TransformerModel(nn.Module):
    def __init__(self, feature_dim, d_model=256, nhead=4, num_layers=2, dim_feed=256, dropout=0.2, pred_len=96):
        super().__init__()
        self.input_proj = nn.Sequential(nn.Linear(feature_dim, d_model), nn.Dropout(dropout))
        self.layer_norm = nn.LayerNorm(d_model)
        self.pos_enc = PositionalEncoding(d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feed, dropout=dropout, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.output_fc = nn.Sequential(nn.Dropout(dropout), nn.Linear(d_model, pred_len))
    def forward(self, x):
        x = self.layer_norm(self.input_proj(x))
        x = self.pos_enc(x)
        x = self.transformer(x)
        return self.output_fc(x[:, -1, :])

print("âœ… [5ë‹¨ê³„] ëª¨ë¸ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ.")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. í•™ìŠµ/ê²€ì¦ DataLoader ìƒì„± + Oversampling
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
full_ds   = TS_Dataset(df_trainval, input_len, pred_len, feature_cols, max_t0_date, last_label_date)
n_total   = len(full_ds)
n_train   = int(n_total * 0.8)
train_idx = list(range(n_train))
val_idx   = list(range(n_train, n_total))

train_ds  = Subset(full_ds, train_idx)
val_ds    = Subset(full_ds, val_idx)

# ê³ ì˜¨ ìœˆë„ìš° í”Œë˜ê·¸ ê³„ì‚°
high_flags = mark_high_windows(train_idx, full_ds.feat, input_len, high_thr_scaled)
# ìƒ˜í”Œ ê°€ì¤‘ì¹˜: ê³ ì˜¨=2, ë‚˜ë¨¸ì§€=1
sample_weights = np.where(high_flags, 2.0, 1.0)
# WeightedRandomSampler ì •ì˜
sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)

# DataLoader ìƒì„±
batch_size = 32
train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, drop_last=True)
val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)

print("âœ… [6ë‹¨ê³„] ê°€ì¤‘ ìƒ˜í”Œë§ì„ ì ìš©í•œ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ.")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. Transformer ëª¨ë¸ í›ˆë ¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
transformer = TransformerModel(feature_dim=len(feature_cols), pred_len=pred_len).to(device)
optimizer = torch.optim.AdamW(transformer.parameters(), lr=5e-5, weight_decay=1e-5)
criterion = AsymWeightedMSE(low_thr_scaled, high_thr_scaled, w_low=0.2, w_high=2.0)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6)

num_epochs = 40
early_patience = 25
best_val_loss = float('inf')
early_stop_counter = 0

print("\n" + "="*80 + "\nâ–¶ PART 7: Transformer ëª¨ë¸ í›ˆë ¨ ì‹œì‘\n" + "="*80)
for epoch in range(1, num_epochs + 1):
    transformer.train(); train_losses = []
    for x_b, y_b in train_loader:
        x_b, y_b = x_b.to(device), y_b.to(device); optimizer.zero_grad()
        loss = criterion(transformer(x_b), y_b); loss.backward()
        torch.nn.utils.clip_grad_norm_(transformer.parameters(), max_norm=1.0); optimizer.step()
        train_losses.append(loss.item())
    avg_tr_loss = np.mean(train_losses)

    transformer.eval(); val_losses = []
    with torch.no_grad():
        for x_b, y_b in val_loader:
            val_losses.append(criterion(transformer(x_b.to(device)), y_b.to(device)).item())
    avg_val_loss = np.mean(val_losses)

    print(f"  [Ep {epoch:02d}] Train Loss: {avg_tr_loss:.6f} | Val Loss: {avg_val_loss:.6f}")

    scheduler.step(avg_val_loss)
    if avg_val_loss < best_val_loss - 1e-6:
        best_val_loss = avg_val_loss
        torch.save(transformer.state_dict(), MODEL_SAVE_PATH)
        early_stop_counter = 0
    else:
        early_stop_counter += 1
        if early_stop_counter >= early_patience:
            print(f"  -- ì¡°ê¸° ì¢…ë£Œ at epoch {epoch} --")
            break

print(f"\nâœ… [7ë‹¨ê³„] ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ. ìµœì¢… ê²€ì¦ ì†ì‹¤: {best_val_loss:.6f}")

#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8. ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥/í‰ê°€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
print("\n" + "="*80 + "\nâ–¶ PART 8: ì˜ˆì¸¡ ë° ê²°ê³¼ í‰ê°€ ì‹œì‘\n" + "="*80)

results = []
best_model = TransformerModel(feature_dim=len(feature_cols), pred_len=pred_len).to(device)
best_model.load_state_dict(torch.load(MODEL_SAVE_PATH))
best_model.eval()

# 'OT' ì»¬ëŸ¼ì˜ ì›ë˜ ìŠ¤ì¼€ì¼ ì •ë³´ë¥¼ ì—­ë³€í™˜ì— ì‚¬ìš©
ot_col_index = feature_cols.index('OT')
ot_min, ot_max = scaler.data_min_[ot_col_index], scaler.data_max_[ot_col_index]

for _, row in submit_df_raw.iterrows():
    t0 = pd.to_datetime(row[0])
    input_end   = t0 - timedelta(hours=1)
    input_start = input_end - timedelta(hours=input_len - 1)

    seq = scaled_df.loc[input_start:input_end, feature_cols].values
    x_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0).to(device)
    with torch.no_grad():
        pred_scaled = best_model(x_tensor).cpu().numpy().flatten()
    
    pred_unnorm = pred_scaled * (ot_max - ot_min) + ot_min
    results.append([row[0]] + pred_unnorm.tolist())

col_names = [submit_df_raw.columns[0]] + [f"T{i}" for i in range(pred_len)]
df_submit = pd.DataFrame(results, columns=col_names)
df_submit.to_csv(SUBMISSION_CSV_PATH, index=False)
print(f"âœ… ì˜ˆì¸¡ íŒŒì¼ ìƒì„± ì™„ë£Œ: {SUBMISSION_CSV_PATH}")

# --- ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™” ---
y_pred_flat = df_submit.drop(columns=[submit_df_raw.columns[0]]).values.flatten()
submit_times = pd.to_datetime(df_submit.iloc[:,0].values)
gt_ots = []
for t in submit_times:
    gt_range = pd.date_range(start=t, periods=pred_len, freq='H')
    # --- ìˆ˜ì •ëœ ë¶€ë¶„ ---
    # 'df_raw' ëŒ€ì‹  'df'ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë‹µ ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    gt_series = df.reindex(gt_range)['OT']
    # --- ìˆ˜ì • ë ---
    gt_ots.append(gt_series.values)
gt_ots_flat = np.concatenate(gt_ots)
valid_indices = ~np.isnan(gt_ots_flat)
gt_ots_clean = gt_ots_flat[valid_indices]
y_pred_clean = y_pred_flat[valid_indices]

# ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
mae = mean_absolute_error(gt_ots_clean, y_pred_clean)
mse = mean_squared_error(gt_ots_clean, y_pred_clean)
rmse = np.sqrt(mse)
r2 = r2_score(gt_ots_clean, y_pred_clean)
print("\n  ã€ ìµœì¢… ì„±ëŠ¥ ì§€í‘œ ã€‘")
print(f"  - MSE  : {mse:.4f} (Kaggle í‰ê°€ì§€í‘œ)")
print(f"  - MAE  : {mae:.4f}")
print(f"  - RMSE : {rmse:.4f}")
print(f"  - RÂ²   : {r2:.4f}")

# íˆìŠ¤í† ê·¸ë¨ ìƒì„±
plt.figure(figsize=(12, 7))
plt.hist(gt_ots_clean, bins=50, alpha=0.7, label='Ground Truth', color='royalblue', density=True)
plt.hist(y_pred_clean, bins=50, alpha=0.7, label='Prediction', color='orangered', density=True)
plt.title("Transformer Prediction vs. Ground Truth Distribution", fontsize=16)
plt.xlabel("OT Value", fontsize=12); plt.ylabel("Density", fontsize=12)
plt.legend(); plt.grid(axis='y', linestyle='--', alpha=0.7); plt.tight_layout()
plt.savefig(HIST_IMG_PATH)
plt.close()
print(f"\nâœ… ë¹„êµ íˆìŠ¤í† ê·¸ë¨ ì €ì¥ ì™„ë£Œ: {HIST_IMG_PATH}")

print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ‰")

# ==============================================================================
# [STEP 4] ìµœì¢… ë¸”ë Œë”©/í›„ì²˜ë¦¬/ìŠ¤ë¬´ë”©/í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (final_blending_results/)
# ==============================================================================
# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¯¸ë¦¬ ìƒì„±ëœ ë‘ ê°œì˜ ì˜ˆì¸¡ íŒŒì¼ì„ ë¶ˆëŸ¬ì™€
# ë™ì  ë¸”ë Œë”©, ì•µì»¤ë§, ìŠ¤ë¬´ë”© í›„ì²˜ë¦¬ë¥¼ í†µí•´ ìµœì¢… ì œì¶œ íŒŒì¼ì„ ìƒì„±
#
# ì‹¤í–‰ ì „ ì¤€ë¹„ë¬¼:
# - ì´ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ í´ë”ì— ETTh1.csv íŒŒì¼ ìœ„ì¹˜
# - ì´ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ í´ë”ì— ì´ì „ ë‹¨ê³„ë“¤ì—ì„œ ìƒì„±ëœ ê²°ê³¼ í´ë”ë“¤ ìœ„ì¹˜
#   (ì˜ˆ: transformer_results/, dlinear_ensemble_results/)
# ==================================================================

import pandas as pd
import numpy as np
import os
from sklearn.metrics import mean_squared_error
import time

print("âœ… [1ë‹¨ê³„] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ê¸°ë³¸ ì„¤ì • ì™„ë£Œ.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ê²½ë¡œ ì„¤ì • (ìƒëŒ€ ê²½ë¡œ ë°©ì‹)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BASE_PATH = os.getcwd()

# ì…ë ¥ ê²°ê³¼ í´ë” ì´ë¦„(ìˆ˜ì • í•„ìš”ì‹œ ì•„ë˜ë§Œ ê³ ì¹˜ë©´ ë¨)
TRANSFORMER_RESULTS_FOLDER = "transformer_results"
DLINEAR_ENSEMBLE_RESULTS_FOLDER = "dlinear_ensemble_results"

transformer_pred_path = os.path.join(BASE_PATH, TRANSFORMER_RESULTS_FOLDER, "submission_transformer.csv")
dlinear_ensemble_path = os.path.join(BASE_PATH, DLINEAR_ENSEMBLE_RESULTS_FOLDER, "submission_conditional_ensemble.csv")
ground_truth_path = os.path.join(BASE_PATH, "ETTh1.csv")

output_dir = os.path.join(BASE_PATH, "final_blending_results/")
os.makedirs(output_dir, exist_ok=True)

print(f"ğŸ“‚ Transformer ì˜ˆì¸¡ íŒŒì¼ ê²½ë¡œ: {transformer_pred_path}")
print(f"ğŸ“‚ DLinear ì•™ìƒë¸” ì˜ˆì¸¡ íŒŒì¼ ê²½ë¡œ: {dlinear_ensemble_path}")
print(f"ğŸ“‚ ìµœì¢… ê²°ê³¼ë¬¼ ì €ì¥ ê²½ë¡œ: {output_dir}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„(Grid) ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
short_term_weights_A = [0.4, 0.5, 0.6]  # ë‹¨ê¸° ì˜ˆì¸¡ì—ì„œ Transformer(A)ì— ë¶€ì—¬í•  ê°€ì¤‘ì¹˜
long_term_weights_A = [0.6, 0.7, 0.8]   # ì¥ê¸° ì˜ˆì¸¡ì—ì„œ Transformer(A)ì— ë¶€ì—¬í•  ê°€ì¤‘ì¹˜
strong_smoothing_configs = [
    [0.8, 0.5, 0.3],
    [0.7, 0.4, 0.2],
    [0.9, 0.6, 0.4]
]

print("\n" + "="*80)
print("â–¶ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
print(f"â–¶ ì´ {len(short_term_weights_A) * len(long_term_weights_A) * len(strong_smoothing_configs)}ê°œì˜ ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
print("="*80 + "\n")

try:
    # ë°ì´í„° ë¡œë“œ
    pred_A = pd.read_csv(transformer_pred_path)
    pred_B = pd.read_csv(dlinear_ensemble_path)
    df_true = pd.read_csv(ground_truth_path, parse_dates=['date'], index_col='date')
    date_col_name = pred_A.columns[0]
    
    gt_ots_list = []
    submit_times = pd.to_datetime(pred_A[date_col_name].values)
    for t in submit_times:
        gt_range = pd.date_range(start=t, periods=96, freq='H')
        gt_data = df_true.reindex(gt_range)['OT'].values
        gt_ots_list.append(gt_data)
    gt_ots_flat = np.concatenate(gt_ots_list)
    valid_indices = ~np.isnan(gt_ots_flat)
    gt_ots_clean = gt_ots_flat[valid_indices]

    print("âœ… ì˜ˆì¸¡ íŒŒì¼ ë° Ground Truth ë°ì´í„° ë¡œë“œ ì™„ë£Œ.")

    results_log = []
    best_mse = float('inf')
    best_params = {}
    run_count = 0
    start_time = time.time()
    df_final = pd.DataFrame() # ìµœê³  ì„±ëŠ¥ dfë¥¼ ì €ì¥í•  ë³€ìˆ˜ ì´ˆê¸°í™”

    for short_w_A in short_term_weights_A:
        for long_w_A in long_term_weights_A:
            for strong_smooth in strong_smoothing_configs:
                run_count += 1
                current_params = {
                    'short_A': short_w_A, 'long_A': long_w_A, 'strong_smooth': strong_smooth
                }
                print(f"--- [ì‹¤í–‰ {run_count}] íŒŒë¼ë¯¸í„°: {current_params} ---")
                # 1. ë™ì  ë¸”ë Œë”©
                short_w_B = 1 - short_w_A
                long_w_B = 1 - long_w_A
                short_term_blended = pred_A.iloc[:, 1:25].values * short_w_A + pred_B.iloc[:, 1:25].values * short_w_B
                long_term_blended = pred_A.iloc[:, 25:].values * long_w_A + pred_B.iloc[:, 25:].values * long_w_B
                blended_values = np.hstack([short_term_blended, long_term_blended])
                df_blended = pd.DataFrame(blended_values, columns=pred_A.columns[1:])
                df_blended.insert(0, date_col_name, pred_A[date_col_name])
                # 2. ì•µì»¤ë§ ë° ìŠ¤ë¬´ë”© í›„ì²˜ë¦¬
                final_predictions_list = []
                for _, row in df_blended.iterrows():
                    t0_timestamp = pd.to_datetime(row[date_col_name])
                    last_known_timestamp = t0_timestamp - pd.Timedelta(hours=1)
                    new_preds = row.drop(date_col_name).values.copy()
                    if last_known_timestamp in df_true.index:
                        anchor_value = df_true.loc[last_known_timestamp]['OT']
                        new_preds[0] = anchor_value
                        for i, weight in enumerate(strong_smooth):
                            if i + 1 < len(new_preds):
                                new_preds[i+1] = (new_preds[i] * weight) + (new_preds[i+1] * (1 - weight))
                    final_predictions_list.append(new_preds)
                # 3. ì„±ëŠ¥(MSE) ê³„ì‚°
                pred_flat = np.concatenate([p.flatten() for p in final_predictions_list])
                y_pred_clean = pred_flat[valid_indices]
                mse = mean_squared_error(gt_ots_clean, y_pred_clean)
                print(f"  â–¶ MSE ê²°ê³¼: {mse:.6f}")
                # 4. ê²°ê³¼ ê¸°ë¡
                log_entry = current_params.copy()
                log_entry['MSE'] = mse
                results_log.append(log_entry)
                # 5. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´ ì—…ë°ì´íŠ¸
                if mse < best_mse:
                    best_mse = mse
                    best_params = current_params
                    print(f"    â­ ìƒˆë¡œìš´ ìµœê³  ê¸°ë¡! (MSE: {best_mse:.6f})")
                    df_final = pd.DataFrame(final_predictions_list, columns=pred_A.columns[1:])
                    df_final.insert(0, date_col_name, pred_A[date_col_name])

except FileNotFoundError as e:
    print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}\nê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”. 'submission_transformer.csv'ì™€ 'submission_conditional_ensemble.csv' íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
except Exception as e:
    print(f"âŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ìµœì¢… ê²°ê³¼ ìš”ì•½ ë° ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if results_log:
    total_time = time.time() - start_time
    print("\n" + "="*80)
    print("â–¶ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"â–¶ ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
    results_df = pd.DataFrame(results_log)
    results_df = results_df.sort_values(by='MSE', ascending=True)
    # ì „ì²´ ê²°ê³¼ ë¡œê·¸ ì €ì¥
    full_log_filename = os.path.join(output_dir, "hyperparam_tuning_log.csv")
    results_df.to_csv(full_log_filename, index=False)
    print(f"âœ… ì „ì²´ íŠœë‹ ë¡œê·¸ê°€ '{full_log_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    # ìµœê³  ì„±ëŠ¥ ì˜ˆì¸¡ íŒŒì¼ ì €ì¥
    if not df_final.empty:
        best_submission_filename = os.path.join(output_dir, "submission_best_tuned.csv")
        df_final.to_csv(best_submission_filename, index=False)
        print(f"âœ… ìµœì  ì˜ˆì¸¡ íŒŒì¼ì´ '{best_submission_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("\nğŸ† ìµœì¢… íŠœë‹ ê²°ê³¼ (ìƒìœ„ 5ê°œ)")
    print(results_df.head(5).to_string())
    print("\n" + "-"*50)
    print("ğŸš€ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•© ğŸš€")
    print(f"  - ë‹¨ê¸°(A) ê°€ì¤‘ì¹˜: {best_params.get('short_A')}")
    print(f"  - ì¥ê¸°(A) ê°€ì¤‘ì¹˜: {best_params.get('long_A')}")
    print(f"  - ìŠ¤ë¬´ë”© ê³„ìˆ˜: {best_params.get('strong_smooth')}")
    print(f"  - ìµœê³  MSE: {best_mse:.6f}")
    print(f"\nìµœì  ì˜ˆì¸¡ íŒŒì¼ì€ 'submission_best_tuned.csv' ì´ë¦„ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("="*80)
else:
    print("\nâŒ ê·¸ë¦¬ë“œ ì„œì¹˜ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì…ë ¥ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")


# ==============================================================================
# [END] ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì™„ë£Œ
# ==============================================================================

